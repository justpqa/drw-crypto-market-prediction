{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ecff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import multiprocessing\n",
    "max_n_jobs = multiprocessing.cpu_count()\n",
    "print(f\"Maximum n_jobs you can use: {max_n_jobs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2025de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_version = 2\n",
    "# 1 for pc feature, \n",
    "# 2 for label correlation feature # seems to work most consistently\n",
    "# 3 for best features based on combination rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731d3a4",
   "metadata": {},
   "source": [
    "#### Import train data and popular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc4e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(f\"data/cleaned/cleaned_train_{feature_version}.parquet\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_features_train = pd.read_parquet(\"data/cleaned/popular_features_train.parquet\")\n",
    "popular_features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f1fe5",
   "metadata": {},
   "source": [
    "#### Implement some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb40f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First need to split into some fold\n",
    "train_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"])\n",
    "\n",
    "default_cv = 1 \n",
    "# NOTE: default_cv must set to 1 instead of 3 based on consistency with LB score contains 49% of test data\n",
    "\n",
    "def create_cv(train_df, features=None, default_cv=default_cv):\n",
    "    if features is not None:\n",
    "        train_df = train_df[features + [\"timestamp\", \"label\"]]\n",
    "    X_train_arr = []\n",
    "    X_test_arr = []\n",
    "    Y_train_arr = []\n",
    "    Y_test_arr = []\n",
    "    for i in range(default_cv):\n",
    "        # train_month = list(range(3, 6 + 3 * i))\n",
    "        train_month = [3, 4, 5, 6, 7, 8]\n",
    "        # test_month = list(range(6 + 3 * i, 9 + 3 * i)) if i < 2 else [12, 1, 2]\n",
    "        test_month = [9, 10, 11, 12, 1, 2]\n",
    "        train = train_df[train_df[\"timestamp\"].dt.month.isin(train_month)] \n",
    "        test = train_df[train_df[\"timestamp\"].dt.month.isin(test_month)]\n",
    "        X_train_arr.append(train.drop([\"timestamp\", \"label\"], axis = 1))\n",
    "        X_test_arr.append(test.drop([\"timestamp\", \"label\"], axis = 1))\n",
    "        Y_train_arr.append(train[\"label\"])\n",
    "        Y_test_arr.append(test[\"label\"])  \n",
    "    return X_train_arr, X_test_arr, Y_train_arr, Y_test_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_score(Y_test, Y_pred):\n",
    "    if isinstance(Y_test, pd.Series) or isinstance(Y_test, pd.DataFrame):\n",
    "        Y_test = Y_test.values\n",
    "    if isinstance(Y_pred, pd.Series) or isinstance(Y_pred, pd.DataFrame):\n",
    "        Y_pred = Y_pred.values\n",
    "    Y_test = np.ravel(Y_test)\n",
    "    Y_pred = np.ravel(Y_pred)\n",
    "    return np.corrcoef(Y_test, Y_pred)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_n_trees = 1000\n",
    "default_random_state = 101\n",
    "# Finetuning XGBoost\n",
    "def objective_xgboost(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": default_n_trees,\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1, log = True),\n",
    "        \"verbosity\": 0,\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0, 1),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0, 1),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0, 1),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 100),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 100),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": default_random_state\n",
    "    }\n",
    "\n",
    "    xgbr = XGBRegressor(**params)\n",
    "    cv_pearson = 0\n",
    "\n",
    "    for i in range(default_cv):\n",
    "        X_train, X_test = X_train_arr[i], X_test_arr[i]\n",
    "        Y_train, Y_test = Y_train_arr[i], Y_test_arr[i]\n",
    "        xgbr.fit(X_train, Y_train)\n",
    "        Y_pred = xgbr.predict(X_test)\n",
    "        cv_pearson += pearson_score(Y_test, Y_pred)\n",
    "    \n",
    "    return cv_pearson / default_cv\n",
    "\n",
    "def objective_lightgbm(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": default_n_trees,\n",
    "        \"verbosity\": -1,\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0, 1),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 100),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 100),\n",
    "        \"random_state\": default_random_state\n",
    "    }\n",
    "\n",
    "    lgbr = LGBMRegressor(**params)\n",
    "    cv_pearson = 0\n",
    "\n",
    "    for i in range(default_cv):\n",
    "        X_train, X_test = X_train_arr[i], X_test_arr[i]\n",
    "        Y_train, Y_test = Y_train_arr[i], Y_test_arr[i]\n",
    "        lgbr.fit(X_train, Y_train)\n",
    "        Y_pred = lgbr.predict(X_test)\n",
    "        cv_pearson += pearson_score(Y_test, Y_pred)\n",
    "    \n",
    "    return cv_pearson / default_cv\n",
    "\n",
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        \"iterations\": default_n_trees,\n",
    "        \"verbose\": False,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 600),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 100),\n",
    "        \"random_seed\": default_random_state\n",
    "    }\n",
    "\n",
    "    cbr = CatBoostRegressor(**params)\n",
    "    cv_pearson = 0\n",
    "\n",
    "    for i in range(default_cv):\n",
    "        X_train, X_test = X_train_arr[i], X_test_arr[i]\n",
    "        Y_train, Y_test = Y_train_arr[i], Y_test_arr[i]\n",
    "        cbr.fit(X_train, Y_train)\n",
    "        Y_pred = cbr.predict(X_test)\n",
    "        cv_pearson += pearson_score(Y_test, Y_pred)\n",
    "    \n",
    "    return cv_pearson / default_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_n_trials = 100\n",
    "default_n_jobs = 4\n",
    "\n",
    "def optimize_xgboost(study_name, storage_name, objective_function=objective_xgboost, n_trials = default_n_trials, n_jobs = default_n_jobs):\n",
    "    print(\"Conduct hyperparam opt for XGBoost\")\n",
    "    study = optuna.create_study(\n",
    "        study_name = study_name,\n",
    "        direction ='maximize',\n",
    "        storage = f\"sqlite:///{storage_name}.db\",\n",
    "        sampler = RandomSampler(seed = 101),\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective_function, n_trials=n_trials, n_jobs=n_jobs)\n",
    "    print('Best hyperparameters:', study.best_params)\n",
    "    print('Best Pearson score:', study.best_value)\n",
    "    return study.best_params\n",
    "\n",
    "def optimize_lightgbm(study_name, storage_name, objective_function=objective_lightgbm, n_trials = default_n_trials, n_jobs = default_n_jobs):\n",
    "    print(\"Conduct hyperparam opt for LightGBM\")\n",
    "    study = optuna.create_study(\n",
    "        study_name = study_name,\n",
    "        direction='maximize',\n",
    "        storage = f\"sqlite:///{storage_name}.db\",\n",
    "        sampler = RandomSampler(seed = 101),\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective_function, n_trials=n_trials, n_jobs=n_jobs)\n",
    "    print('Best hyperparameters:', study.best_params)\n",
    "    print('Best Pearson score:', study.best_value)\n",
    "    return study.best_params\n",
    "\n",
    "def optimize_catboost(study_name, storage_name, objective_function=objective_catboost, n_trials = default_n_trials, n_jobs = default_n_jobs):\n",
    "    print(\"Conduct hyperparam opt for CatBoost\")\n",
    "    study = optuna.create_study(\n",
    "        study_name = study_name,\n",
    "        direction='maximize',\n",
    "        storage = f\"sqlite:///{storage_name}.db\",\n",
    "        sampler = RandomSampler(seed = 101),\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective_function, n_trials=n_trials, n_jobs=n_jobs)\n",
    "    print('Best hyperparameters:', study.best_params)\n",
    "    print('Best Pearson score:', study.best_value)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54fd743",
   "metadata": {},
   "source": [
    "#### First iteration: training with all features from the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5acc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_added_df = pd.concat([train_df, popular_features_train], axis=1)\n",
    "\n",
    "X_train_arr, X_test_arr, Y_train_arr, Y_test_arr = create_cv(train_added_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb106a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgboost = optimize_xgboost(\n",
    "    f\"xgboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_study\",\n",
    "    f\"xgboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_study\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lightgbm = optimize_lightgbm(\n",
    "    f\"lightgbm_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_study\",\n",
    "    f\"lightgbm_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_study\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_catboost = optimize_catboost(\n",
    "    f\"catboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_study\",\n",
    "    f\"catboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_study\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e2b50",
   "metadata": {},
   "source": [
    "Analyze params - cv relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8801e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_study_df(filename):\n",
    "    study = optuna.load_study(\n",
    "        study_name = filename,\n",
    "        storage = f\"sqlite:///{filename}.db\"\n",
    "    )\n",
    "    study_df = []\n",
    "    for trial in study.trials:\n",
    "        trial_dict = trial.params\n",
    "        trial_dict[\"value\"] = trial.value\n",
    "        study_df.append(trial_dict)\n",
    "\n",
    "    return pd.DataFrame(study_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d57f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_value_viz(study_df):\n",
    "    nrows = (study_df.shape[1] - 1) // 3 + ((study_df.shape[1] - 1) % 3 > 0)\n",
    "    fig, ax = plt.subplots(nrows = nrows, ncols = 3, figsize = (14, 5 * nrows))\n",
    "    for inx, var in enumerate(study_df.columns):\n",
    "        x, y = inx // 3, inx % 3\n",
    "        if var != \"value\":\n",
    "            sns.scatterplot(study_df, x = var, y = \"value\", ax = ax[x][y])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d81cc8",
   "metadata": {},
   "source": [
    "Analyze feature importance + CV performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccea6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params_from_file(filename):\n",
    "    study = optuna.load_study(\n",
    "        study_name = filename,\n",
    "        storage = f\"sqlite:///{filename}.db\"\n",
    "    )\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": default_n_trees,\n",
    "    \"verbosity\": 0,\n",
    "    \"enable_categorical\": True,\n",
    "    \"random_state\": default_random_state\n",
    "}\n",
    "best_params_xgboost = get_best_params_from_file(f\"xgboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_study\")\n",
    "for p in best_params_xgboost:\n",
    "    params[p] = best_params_xgboost[p]\n",
    "\n",
    "xgboost_feature_importances = {}\n",
    "\n",
    "xgbr = XGBRegressor(**params)\n",
    "for i in range(default_cv):\n",
    "    X_train, X_test = X_train_arr[i], X_test_arr[i]\n",
    "    Y_train, Y_test = Y_train_arr[i], Y_test_arr[i]\n",
    "    xgbr.fit(X_train, Y_train)\n",
    "    print(pearson_score(Y_test, xgbr.predict(X_test)))\n",
    "    features = xgbr.feature_names_in_.tolist()\n",
    "    features_i = xgbr.feature_importances_.tolist()\n",
    "    for inx, feat in enumerate(features):\n",
    "        xgboost_feature_importances[feat] = xgboost_feature_importances.get(feat, 0) + features_i[inx]\n",
    "\n",
    "# print(feature_importances)\n",
    "plt.hist(xgboost_feature_importances.values())\n",
    "# Seems like only COD features are important (can try to only use 4-8 hours if 4-13 hours does not work well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f for f in xgboost_feature_importances if xgboost_feature_importances[f] > 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a236e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f for f in xgboost_feature_importances if xgboost_feature_importances[f] > 0.015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca63d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": default_n_trees,\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": default_random_state\n",
    "}\n",
    "best_params_lightgbm = get_best_params_from_file(f\"lightgbm_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_study\")\n",
    "for p in best_params_lightgbm:\n",
    "    params[p] = best_params_lightgbm[p]\n",
    "\n",
    "lightgbm_feature_importances = {}\n",
    "\n",
    "lgbr = LGBMRegressor(**params)\n",
    "for i in range(default_cv):\n",
    "    X_train, X_test = X_train_arr[i], X_test_arr[i]\n",
    "    Y_train, Y_test = Y_train_arr[i], Y_test_arr[i]\n",
    "    lgbr.fit(X_train, Y_train)\n",
    "    print(pearson_score(Y_test, lgbr.predict(X_test)))\n",
    "    features = lgbr.feature_names_in_.tolist()\n",
    "    features_i = lgbr.feature_importances_.tolist()\n",
    "    for inx, feat in enumerate(features):\n",
    "        lightgbm_feature_importances[feat] = lightgbm_feature_importances.get(feat, 0) + features_i[inx]\n",
    "\n",
    "plt.hist(lightgbm_feature_importances.values())\n",
    "# seems to pick up time features not as good as past 4 hours features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7366dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f for f in lightgbm_feature_importances if lightgbm_feature_importances[f] >= 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f for f in lightgbm_feature_importances if lightgbm_feature_importances[f] >= 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"iterations\": default_n_trees,\n",
    "    \"verbose\": False,\n",
    "    \"random_seed\": default_random_state\n",
    "}\n",
    "best_params_catboost = get_best_params_from_file(f\"catboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_study\")\n",
    "for p in best_params_catboost:\n",
    "    params[p] = best_params_catboost[p]\n",
    "\n",
    "catboost_feature_importances = {}\n",
    "\n",
    "cbr = CatBoostRegressor(**params)\n",
    "cv_rmse = 0\n",
    "\n",
    "for i in range(default_cv):\n",
    "    X_train, X_test = X_train_arr[i], X_test_arr[i]\n",
    "    Y_train, Y_test = Y_train_arr[i], Y_test_arr[i]\n",
    "    cbr.fit(X_train, Y_train)\n",
    "    print(pearson_score(Y_test, cbr.predict(X_test)))\n",
    "    features = cbr.feature_names_\n",
    "    features_i = cbr.feature_importances_.tolist()\n",
    "    for inx, feat in enumerate(features):\n",
    "        catboost_feature_importances[feat] = catboost_feature_importances.get(feat, 0) + features_i[inx]\n",
    "\n",
    "plt.hist(catboost_feature_importances.values())\n",
    "# can pick up a combination of both past cod and tss, not good at picking up ph, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f for f in catboost_feature_importances if catboost_feature_importances[f] >= 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8120d",
   "metadata": {},
   "source": [
    "Get top 20 important features in all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_feature_importances_df = pd.DataFrame(\n",
    "    {\"var\": xgboost_feature_importances.keys(), \"importance\": xgboost_feature_importances.values()}\n",
    ")\n",
    "xgboost_feature_importances_df[\"rank_importance_xgboost\"] = xgboost_feature_importances_df[\"importance\"].rank(ascending=False)\n",
    "lightgbm_feature_importances_df = pd.DataFrame(\n",
    "    {\"var\": lightgbm_feature_importances.keys(), \"importance\": lightgbm_feature_importances.values()}\n",
    ")\n",
    "lightgbm_feature_importances_df[\"rank_importance_lightgbm\"] = lightgbm_feature_importances_df[\"importance\"].rank(ascending=False)\n",
    "catboost_feature_importances_df = pd.DataFrame(\n",
    "    {\"var\": catboost_feature_importances.keys(), \"importance\": catboost_feature_importances.values()}\n",
    ")\n",
    "catboost_feature_importances_df[\"rank_importance_catboost\"] = catboost_feature_importances_df[\"importance\"].rank(ascending=False)\n",
    "feature_importances_df = xgboost_feature_importances_df.merge(\n",
    "    lightgbm_feature_importances_df,\n",
    "    on=\"var\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "feature_importances_df = feature_importances_df.merge(\n",
    "    catboost_feature_importances_df,\n",
    "    on=\"var\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "feature_importances_df = feature_importances_df[[\"var\", \"rank_importance_xgboost\", \"rank_importance_lightgbm\", \"rank_importance_catboost\"]]\n",
    "feature_importances_df[\"rank\"] = 1/3 * (feature_importances_df[\"rank_importance_xgboost\"] + feature_importances_df[\"rank_importance_lightgbm\"] + feature_importances_df[\"rank_importance_catboost\"])\n",
    "feature_importances_df = feature_importances_df.sort_values(by=\"rank\", ascending=True).reset_index().drop(\"index\", axis = 1)\n",
    "feature_importances_df[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8dc1e",
   "metadata": {},
   "source": [
    "#### Second iteration: a more truncated version from the first collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f34e76",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56532466",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_importance_threshold = 0.011\n",
    "xgboost_best_features = [\n",
    "    f for f in xgboost_feature_importances if xgboost_feature_importances[f] > xgboost_importance_threshold\n",
    "] + [\"volume\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n",
    "print(len(xgboost_best_features))\n",
    "train_added_df = pd.concat([train_df, popular_features_train], axis=1)\n",
    "\n",
    "X_train_arr, X_test_arr, Y_train_arr, Y_test_arr = create_cv(train_added_df, xgboost_best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgboost_params_truncated = optimize_xgboost(\n",
    "    f\"xgboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_truncated_study\",\n",
    "    f\"xgboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_truncated_study\"\n",
    ") # much worse than using all features  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13ba89",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ef415",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_importance_threshold = 20\n",
    "lightgbm_best_features = [\n",
    "    f for f in lightgbm_feature_importances if lightgbm_feature_importances[f] > lightgbm_importance_threshold\n",
    "] + [\"volume\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n",
    "print(len(lightgbm_best_features))\n",
    "train_added_df = pd.concat([train_df, popular_features_train], axis=1)\n",
    "\n",
    "X_train_arr, X_test_arr, Y_train_arr, Y_test_arr = create_cv(train_added_df, lightgbm_best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lightgbm_params_truncated = optimize_lightgbm(\n",
    "    f\"lightgbm_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_truncated_study\",\n",
    "    f\"lightgbm_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_truncated_study\"\n",
    ")\n",
    "# also much worse "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6953fd8",
   "metadata": {},
   "source": [
    "##### Third Iteration: a common truncated version using good features across all models + popular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = feature_importances_df[feature_importances_df[\"rank\"] <= 25][\"var\"].tolist() + \\\n",
    "                [\"volume\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n",
    "train_added_df = pd.concat([train_df, popular_features_train], axis=1)\n",
    "X_train_arr, X_test_arr, Y_train_arr, Y_test_arr = create_cv(train_added_df, best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c7f75",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ed407",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgboost_params_common_truncated = optimize_xgboost(\n",
    "    f\"xgboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_study\",\n",
    "    f\"xgboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_study\"\n",
    ") # work almost as good as all features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534cb1d",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c656479",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lightgbm_params_common_truncated = optimize_lightgbm(\n",
    "    f\"lightgbm_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_study\",\n",
    "    f\"lightgbm_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_study\"\n",
    ")\n",
    "# slightly worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb7723c",
   "metadata": {},
   "source": [
    "Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77286723",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_catboost_params_common_truncated = optimize_catboost(\n",
    "    f\"catboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_study\",\n",
    "    f\"catboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_study\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
