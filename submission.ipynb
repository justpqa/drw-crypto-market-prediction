{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158f9a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justpqa/drw-crypto-market-prediction/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum n_jobs you can use: 12\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import multiprocessing\n",
    "max_n_jobs = multiprocessing.cpu_count()\n",
    "print(f\"Maximum n_jobs you can use: {max_n_jobs}\")\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nnmx\n",
    "import mlx.optimizers as optimmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779f7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_version = 2\n",
    "default_cv = 4\n",
    "# 1 for pc feature, \n",
    "# 2 for label correlation feature\n",
    "# 3 for best features based on combination rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45976d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_n_trees = 1000\n",
    "default_random_state = 101\n",
    "default_num_layers = 2\n",
    "random.seed(default_random_state)\n",
    "np.random.seed(default_random_state)\n",
    "mx.random.seed(default_random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7d631",
   "metadata": {},
   "source": [
    "### Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498f9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_features_train = pd.read_parquet(\"data/cleaned/popular_features_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f985312",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(f\"data/cleaned/cleaned_train_{feature_version}.parquet\")\n",
    "train_df = pd.concat([train_df, popular_features_train], axis = 1)\n",
    "train_df[\"__index_level_0__\"] = pd.to_datetime(train_df[\"__index_level_0__\"])\n",
    "# train_df = train_df[train_df[\"__index_level_0__\"].dt.month.isin([12, 1, 2])].reset_index().drop(\"index\", axis = 1)\n",
    "# X_train = train_df.drop(columns=[\"__index_level_0__\", \"label\"])\n",
    "# Y_train = train_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c875ae5",
   "metadata": {},
   "source": [
    "### Train best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9b1a9",
   "metadata": {},
   "source": [
    "#### Get best features and training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2aebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['X757', 'X758', 'X759', 'X508', 'X614', 'X752', 'X331', 'X445', 'X465', 'X385', \n",
    "                 'X466', 'X95', 'X23', 'X219', 'X31', 'X373', 'X379', 'X284', 'X750', 'X652', \n",
    "                 'X279', 'X89', 'X169', 'X753', 'X226', 'X28', 'X444', 'X272', 'X271', 'X218']\n",
    "best_features = list(set(best_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b295efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2],\n",
       " [3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
       " [4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " [5, 6, 7, 8, 9, 10, 11, 12, 1],\n",
       " [6, 7, 8, 9, 10, 11, 12, 1, 2],\n",
       " [3, 4, 5, 6, 7, 8, 9, 10],\n",
       " [4, 5, 6, 7, 8, 9, 10, 11],\n",
       " [5, 6, 7, 8, 9, 10, 11, 12],\n",
       " [6, 7, 8, 9, 10, 11, 12, 1],\n",
       " [7, 8, 9, 10, 11, 12, 1, 2],\n",
       " [3, 4, 5, 6, 7, 8],\n",
       " [4, 5, 6, 7, 8, 9],\n",
       " [5, 6, 7, 8, 9, 10],\n",
       " [6, 7, 8, 9, 10, 11],\n",
       " [7, 8, 9, 10, 11, 12],\n",
       " [8, 9, 10, 11, 12, 1],\n",
       " [9, 10, 11, 12, 1, 2],\n",
       " [3, 4, 5, 6],\n",
       " [4, 5, 6, 7],\n",
       " [5, 6, 7, 8],\n",
       " [6, 7, 8, 9],\n",
       " [7, 8, 9, 10],\n",
       " [8, 9, 10, 11],\n",
       " [9, 10, 11, 12],\n",
       " [10, 11, 12, 1],\n",
       " [11, 12, 1, 2],\n",
       " [3, 4, 5],\n",
       " [4, 5, 6],\n",
       " [5, 6, 7],\n",
       " [6, 7, 8],\n",
       " [7, 8, 9],\n",
       " [8, 9, 10],\n",
       " [9, 10, 11],\n",
       " [10, 11, 12],\n",
       " [11, 12, 1],\n",
       " [12, 1, 2]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2]\n",
    "training_timeframe = [] \n",
    "# [12, 9, 8, 6, 4, 3] is currently best one\n",
    "window_sizes = [12, 9, 8, 6, 4, 3]\n",
    "for window_size in window_sizes:\n",
    "    for i in range(13 - window_size):\n",
    "        training_timeframe.append(months[i: i + window_size])\n",
    "training_timeframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82af05",
   "metadata": {},
   "source": [
    "#### Utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4885c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params_from_file(filename):\n",
    "    study = optuna.load_study(\n",
    "        study_name = filename,\n",
    "        storage = f\"sqlite:///{filename}.db\"\n",
    "    )\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64549233",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a05045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [01:17<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": default_n_trees,\n",
    "    \"verbosity\": 0,\n",
    "    \"enable_categorical\": True,\n",
    "    \"random_state\": default_random_state\n",
    "}\n",
    "best_params_xgboost = get_best_params_from_file(f\"xgboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_20_study\")\n",
    "for p in best_params_xgboost:\n",
    "    params[p] = best_params_xgboost[p]\n",
    "\n",
    "xgbr_arr = []\n",
    "\n",
    "for i in tqdm(range(len(training_timeframe))):\n",
    "    temp = deepcopy(train_df)\n",
    "    temp = temp[temp[\"__index_level_0__\"].dt.month.isin(training_timeframe[i])].reset_index().drop(\"index\", axis = 1)\n",
    "    X_train = temp.drop(columns=[\"__index_level_0__\", \"label\"])\n",
    "    X_train = X_train[best_features]\n",
    "    Y_train = temp[\"label\"]\n",
    "    xgbr = XGBRegressor(**params)\n",
    "    xgbr.fit(X_train, Y_train)\n",
    "    xgbr_arr.append(xgbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915f46d",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b14c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": default_n_trees,\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": default_random_state,\n",
    "}\n",
    "best_params_lightgbm = get_best_params_from_file(f\"lightgbm_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_20_study\")\n",
    "for p in best_params_lightgbm:\n",
    "    params[p] = best_params_lightgbm[p]\n",
    "\n",
    "lgbr_arr = []\n",
    "\n",
    "for i in tqdm(range(len(training_timeframe))):\n",
    "    temp = deepcopy(train_df)\n",
    "    temp = temp[temp[\"__index_level_0__\"].dt.month.isin(training_timeframe[i])].reset_index().drop(\"index\", axis = 1)\n",
    "    X_train = temp.drop(columns=[\"__index_level_0__\", \"label\"])\n",
    "    X_train = X_train[best_features]\n",
    "    Y_train = temp[\"label\"]\n",
    "    lgbr = LGBMRegressor(**params)\n",
    "    lgbr.fit(X_train, Y_train)\n",
    "    lgbr_arr.append(lgbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc5ede8",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     \"iterations\": default_n_trees,\n",
    "#     \"verbose\": False,\n",
    "#     \"random_seed\": default_random_state\n",
    "# }\n",
    "# best_params_catboost = get_best_params_from_file(f\"catboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_20_study\")\n",
    "# for p in best_params_catboost:\n",
    "#     params[p] = best_params_catboost[p]\n",
    "\n",
    "# cbr_arr = []\n",
    "\n",
    "# for i in tqdm(range(len(training_timeframe))):\n",
    "#     temp = deepcopy(train_df)\n",
    "#     temp = temp[temp[\"__index_level_0__\"].dt.month.isin(training_timeframe[i])].reset_index().drop(\"index\", axis = 1)\n",
    "#     X_train = temp.drop(columns=[\"__index_level_0__\", \"label\"])\n",
    "#     X_train = X_train[best_features]\n",
    "#     Y_train = temp[\"label\"]\n",
    "#     cbr = CatBoostRegressor(**params)\n",
    "#     cbr.fit(X_train, Y_train)\n",
    "#     cbr_arr.append(xgbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1482726",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f361db",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8056ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra code to \"reduce\" from float64 to float32\n",
    "def float64_to_float32(data):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        for col in data.columns:\n",
    "            data[col] = data[col].astype(\"float32\")\n",
    "    elif isinstance(data, pd.Series):\n",
    "        data = data.astype(\"float32\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9dacf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_cv_to_mlx_cv(X_train, Y_train = None):\n",
    "    # Normalize forst\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.values)\n",
    "\n",
    "    # Convert to mlx format\n",
    "    X_train = mx.array(X_train)\n",
    "    if Y_train is not None:\n",
    "        Y_train = mx.array(Y_train.values)\n",
    "        return X_train, Y_train\n",
    "    else:\n",
    "        return X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40f197",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2c395bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# We do not use the reset method this time so you have to create the model at each fold\n",
    "class MLPMLX(nnmx.Module):\n",
    "    def __init__(self, num_features, hidden_layers_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers & batchnorm\n",
    "        last_layer = num_features\n",
    "        self.layers = []\n",
    "        for current_layer in hidden_layers_size:\n",
    "            self.layers.append(nnmx.Linear(last_layer, current_layer))\n",
    "            last_layer = current_layer\n",
    "        self.layers.append(nnmx.Linear(last_layer, 1))\n",
    "\n",
    "        # Initialize activation\n",
    "        self.activation = nnmx.ReLU()\n",
    "\n",
    "        # Initialze dropout\n",
    "        self.dropout = nnmx.Dropout(p = dropout)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for inx, layer in enumerate(self.layers):\n",
    "            if inx == len(self.layers) - 1:\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                x = self.activation(x)\n",
    "                x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7856fc",
   "metadata": {},
   "source": [
    "Define batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70462fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for batch iteration\n",
    "def batch_iterate(batch_size, X, Y = None, shuffle = True):\n",
    "    if Y is not None:\n",
    "        for i in range(0, Y.size, batch_size):\n",
    "            X_curr = X[i: min(i + batch_size, Y.size), :]\n",
    "            Y_curr = Y[i: min(i + batch_size, Y.size)]\n",
    "            if shuffle:\n",
    "                inx_lst = mx.random.permutation(batch_size)\n",
    "                X_curr = X_curr[inx_lst, :]\n",
    "                Y_curr = Y_curr[inx_lst]\n",
    "            yield X_curr, Y_curr\n",
    "    else:\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_curr = X[i: min(i + batch_size, X.shape[0]), :]\n",
    "            yield X_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988601b",
   "metadata": {},
   "source": [
    "Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "868b1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate function for train & eval step\n",
    "def train_mlp_mlx(model, loss_and_grad_fn, optimizer, X_train, Y_train, batch_size, num_epochs):\n",
    "    model.train()\n",
    "    for _ in tqdm(range(num_epochs)):\n",
    "        for (inputs, targets) in batch_iterate(batch_size, X_train, Y_train):\n",
    "            _, grads = loss_and_grad_fn(model, inputs, targets)\n",
    "            # Update the optimizer state and model parameters in a single call\n",
    "            optimizer.update(model, grads)\n",
    "            # Force a graph evaluation\n",
    "            mx.eval(model.parameters(), optimizer.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e3d8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_mlx_lst(num_features, hidden_layers_size, dropout, lr, train_df, batch_size, num_epochs, best_features, training_timeframe):\n",
    "    mlp_mlx_arr = []\n",
    "    for _, timeframe in enumerate(training_timeframe):\n",
    "        # Get the data\n",
    "        temp = deepcopy(train_df)\n",
    "        temp = temp[temp[\"__index_level_0__\"].dt.month.isin(timeframe)].reset_index().drop(\"index\", axis = 1)\n",
    "        X_train = temp.drop(columns=[\"__index_level_0__\", \"label\"])\n",
    "        X_train = X_train[best_features]\n",
    "        Y_train = temp[\"label\"]\n",
    "\n",
    "        # Preprocess the data\n",
    "        # convert to float 32\n",
    "        X_train = float64_to_float32(X_train)\n",
    "        Y_train = float64_to_float32(Y_train)\n",
    "        # convert to mlx\n",
    "        X_train, Y_train = normal_cv_to_mlx_cv(X_train, Y_train)\n",
    "        \n",
    "        # initialize the model\n",
    "        mx.random.seed(default_random_state)\n",
    "        model = MLPMLX(num_features, hidden_layers_size, dropout)\n",
    "\n",
    "        # Initialize the loss function\n",
    "        def loss_fn(model, X, Y):\n",
    "            Y_pred = model(X).reshape(-1)\n",
    "            return mx.mean(nnmx.losses.mse_loss(Y_pred, Y))\n",
    "        loss_and_grad_fn = nnmx.value_and_grad(model, loss_fn)\n",
    "\n",
    "        # Reinitialize the optimizer\n",
    "        optimizer = optimmx.Adam(learning_rate = lr)\n",
    "\n",
    "        # Train the model\n",
    "        train_mlp_mlx(model, loss_and_grad_fn, optimizer, X_train, Y_train, batch_size, num_epochs)\n",
    "\n",
    "        # Add the model to model list\n",
    "        mlp_mlx_arr.append(model)\n",
    "    return mlp_mlx_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70040144",
   "metadata": {},
   "source": [
    "Define function for making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "345699e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mlp_mlx(model, X_test, batch_size):\n",
    "    # Preprocess the data\n",
    "    # convert to float 32\n",
    "    X_test = float64_to_float32(X_test)\n",
    "    # convert to mlx\n",
    "    X_test = normal_cv_to_mlx_cv(X_test)\n",
    "    \n",
    "    outputs_all = np.zeros(0)\n",
    "    model.eval()\n",
    "    for inputs in batch_iterate(batch_size, X_test):\n",
    "        outputs = model(inputs).reshape(-1)\n",
    "        # convert back to numpy\n",
    "        outputs = np.array(outputs)\n",
    "        # Load to overall Y_test, Y_pred to calculate pearson score later\n",
    "        outputs_all = np.concatenate([outputs_all, outputs])\n",
    "    return outputs_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f73281",
   "metadata": {},
   "source": [
    "Train our model list based on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d59f0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['X757', 'X758', 'X759', 'X508', 'X614', 'X752', 'X331', 'X445', 'X465', 'X385', \n",
    "                 'X466', 'X95', 'X23', 'X219', 'X31', 'X373', 'X379', 'X284', 'X750', 'X652', \n",
    "                 'X279', 'X89', 'X169', 'X753', 'X226', 'X28', 'X444', 'X272', 'X271', 'X218']\n",
    "best_features = list(set(best_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters for training\n",
    "best_params_mlp_1 = get_best_params_from_file(f\"mlp_mlx_{feature_version}_{default_cv}_{default_random_state}_1_common_truncated_{len(best_features)}_study\")\n",
    "num_features = len(best_features)\n",
    "log_2_hidden_layers_size_1 = []\n",
    "for i in range(1):\n",
    "    log_2_hidden_layers_size_1.append(best_params_mlp_1[f\"log2_hidden_layer_{i}\"])\n",
    "hidden_layers_size_1 = [2**l for l in log_2_hidden_layers_size_1]\n",
    "dropout_1 = best_params_mlp_1[\"dropout\"]\n",
    "lr_1 = best_params_mlp_1[\"lr\"]\n",
    "batch_size_1 = best_params_mlp_1[\"batch_size\"]\n",
    "num_epochs_1 = best_params_mlp_1[\"num_epochs\"]\n",
    "\n",
    "# Conduct training to get model list\n",
    "mlp_mlx_arr_1 = train_mlp_mlx_lst(num_features, hidden_layers_size_1, dropout_1, lr_1, train_df, batch_size_1, num_epochs_1, best_features, training_timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a093f23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.95it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.97it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.95it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.99it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.26it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.66it/s]\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.10it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.09it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  3.16it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  3.05it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  3.08it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  3.08it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  3.02it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  3.11it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.76it/s]\n",
      "100%|██████████| 20/20 [10:48<00:00, 32.40s/it] \n",
      "100%|██████████| 20/20 [00:04<00:00,  4.17it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.96it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.52it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.34it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.42it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.39it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.44it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.75it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.94it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.05it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.01it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.78it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.98it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.86it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.81it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.89it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get parameters for training\n",
    "best_params_mlp_2 = get_best_params_from_file(f\"mlp_mlx_{feature_version}_{default_cv}_{default_random_state}_2_common_truncated_{len(best_features)}_study\")\n",
    "num_features = len(best_features)\n",
    "log_2_hidden_layers_size_2 = []\n",
    "for i in range(2):\n",
    "    log_2_hidden_layers_size_2.append(best_params_mlp_2[f\"log2_hidden_layer_{i}\"])\n",
    "hidden_layers_size_2 = [2**l for l in log_2_hidden_layers_size_2]\n",
    "dropout_2 = best_params_mlp_2[\"dropout\"]\n",
    "lr_2 = best_params_mlp_2[\"lr\"]\n",
    "batch_size_2 = best_params_mlp_2[\"batch_size\"]\n",
    "num_epochs_2 = best_params_mlp_2[\"num_epochs\"]\n",
    "\n",
    "# Conduct training to get model list\n",
    "mlp_mlx_arr_2 = train_mlp_mlx_lst(num_features, hidden_layers_size_2, dropout_2, lr_2, train_df, batch_size_2, num_epochs_2, best_features, training_timeframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd992dd2",
   "metadata": {},
   "source": [
    "### Saving model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4279a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since training of this model is expensive, try to save them somewhere\n",
    "# with open(f\"xgbr_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_{len(best_features)}_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(xgbr_arr, f)\n",
    "# with open(f\"lgbr_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_{len(best_features)}_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(lgbr_arr, f)\n",
    "# with open(f\"cbr_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_{len(best_features)}_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(cbr_arr, f)\n",
    "# with open(f\"mlp_mlx_{feature_version}_{default_cv}_{default_random_state}_{default_num_layers}_common_truncated_{len(best_features)}_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(mlp_mlx_arr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9630cb",
   "metadata": {},
   "source": [
    "### Making prediction & submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb8ee22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_features_test = pd.read_parquet(\"data/cleaned/popular_features_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e43f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import mlp_mlx model if needed\n",
    "# best_features = ['X862', 'X598', 'X863', 'X856', 'X612', 'X466', 'X533', 'X861', 'X445', 'X531', \n",
    "#                      'X385', 'X23', 'X284', 'X465', 'X331', 'X95', 'X285', 'X31', 'X169', 'X137']\n",
    "# with open(f\"xgbr_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_{len(best_features)}_model.pkl\", \"rb\") as f:\n",
    "#     xgbr_arr = pickle.load(f)\n",
    "# with open(f\"lgbr_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_{len(best_features)}_model.pkl\", \"rb\") as f:\n",
    "#     lgbr_arr = pickle.load(f)\n",
    "# with open(f\"cbr_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_{len(best_features)}_model.pkl\", \"rb\") as f:\n",
    "#     cbr_arr = pickle.load(f)\n",
    "# with open(f\"mlp_mlx_{feature_version}_{default_cv}_{default_random_state}_1_common_truncated_{len(best_features)}_model.pkl\", \"rb\") as f:\n",
    "#     mlp_mlx_arr_1 = pickle.load(f)\n",
    "# with open(f\"mlp_mlx_{feature_version}_{default_cv}_{default_random_state}_2_common_truncated_{len(best_features)}_model.pkl\", \"rb\") as f:\n",
    "#     mlp_mlx_arr_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096457bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:20<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_parquet(f\"data/cleaned/cleaned_test_{feature_version}.parquet\")\n",
    "X_test = pd.concat([X_test, popular_features_test], axis = 1)\n",
    "X_test = X_test.drop(columns=[\"label\"])\n",
    "\n",
    "# get X_test for gbdt\n",
    "best_features_gbdt = ['X757', 'X758', 'X759', 'X508', 'X614', 'X752', 'X331', 'X445', 'X465', 'X385', \n",
    "                      'X466', 'X95', 'X23', 'X219', 'X31', 'X373', 'X379', 'X284', 'X750', 'X652', \n",
    "                      'X279', 'X89', 'X169', 'X753', 'X226', 'X28', 'X444', 'X272', 'X271', 'X218']\n",
    "best_features_gbdt = list(set(best_features_gbdt))\n",
    "X_test_gbdt = X_test[best_features_gbdt]\n",
    "\n",
    "# get X_test for mlp\n",
    "best_features_mlp = ['X757', 'X758', 'X759', 'X508', 'X614', 'X752', 'X331', 'X445', 'X465', 'X385', \n",
    "                     'X466', 'X95', 'X23', 'X219', 'X31', 'X373', 'X379', 'X284', 'X750', 'X652', \n",
    "                     'X279', 'X89', 'X169', 'X753', 'X226', 'X28', 'X444', 'X272', 'X271', 'X218']\n",
    "best_features_mlp = list(set(best_features_mlp))\n",
    "X_test_mlp = X_test[best_features_mlp]\n",
    "\n",
    "# conduct prediction\n",
    "Y_pred = np.zeros(X_test.shape[0])\n",
    "n_models = 2\n",
    "for i in tqdm(range(len(training_timeframe))):\n",
    "    Y_pred += (\n",
    "        xgbr_arr[i].predict(X_test_gbdt) + \n",
    "        # lgbr_arr[i].predict(X_test_gbdt) +\n",
    "        predict_mlp_mlx(mlp_mlx_arr_2[i], X_test_mlp, batch_size_2)\n",
    "    ) / n_models\n",
    "Y_pred /= len(training_timeframe)\n",
    "\n",
    "# Not really better than above\n",
    "# Y_pred = np.zeros(X_test.shape[0])\n",
    "# n_models = 2\n",
    "# Y_pred_dict = {w: np.zeros(X_test.shape[0]) for w in window_sizes} # timeframe size - prediction\n",
    "# for i in tqdm(range(len(training_timeframe))):\n",
    "#     Y_pred_dict[len(training_timeframe[i])] += (\n",
    "#         xgbr_arr[i].predict(X_test_gbdt) + \n",
    "#         # lgbr_arr[i].predict(X_test_gbdt) +\n",
    "#         predict_mlp_mlx(mlp_mlx_arr_2[i], X_test_mlp, batch_size_2)\n",
    "#     ) / n_models\n",
    "# for w in Y_pred_dict:\n",
    "#     Y_pred += Y_pred_dict[w] / (12 - w + 1)\n",
    "# Y_pred /= len(window_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79a13fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.001865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.063624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.076649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.096325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.058924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  prediction\n",
       "0   1   -0.001865\n",
       "1   2    0.063624\n",
       "2   3   -0.076649\n",
       "3   4   -0.096325\n",
       "4   5   -0.058924"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": X_test.index + 1,\n",
    "    \"prediction\": Y_pred\n",
    "})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "918e06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951e4a6",
   "metadata": {},
   "source": [
    "Analysis of contribution to submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_shap_values(model, X_test):\n",
    "#     explainer = shap.TreeExplainer(model)\n",
    "#     shap_values = explainer.shap_values(X_test)\n",
    "#     mean_abs_shap = np.mean(np.abs(shap_values), axis = 0)\n",
    "#     return mean_abs_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost_feature_importances = {}\n",
    "# lightgbm_feature_importances = {}\n",
    "\n",
    "# # only consider the fold with whole dataset to be comparable with cv scheme\n",
    "# features = xgbr_arr[0].feature_names_in_.tolist()\n",
    "# features_i = get_shap_values(xgbr_arr[i], X_test)\n",
    "# for inx, feat in enumerate(features):\n",
    "#     xgboost_feature_importances[feat] = xgboost_feature_importances.get(feat, 0) + features_i[inx]\n",
    "# features = lgbr_arr[0].feature_names_in_.tolist()\n",
    "# features_i = get_shap_values(lgbr_arr[i], X_test)\n",
    "# for inx, feat in enumerate(features):\n",
    "#     lightgbm_feature_importances[feat] = lightgbm_feature_importances.get(feat, 0) + features_i[inx]\n",
    "\n",
    "# xgboost_feature_importances_df = pd.DataFrame(\n",
    "#     {\"var\": xgboost_feature_importances.keys(), \"importance\": xgboost_feature_importances.values()}\n",
    "# )\n",
    "# #xgboost_feature_importances_df[\"importance\"] /= len(training_timeframe)\n",
    "# # xgboost_feature_importances_df[\"rank_importance\"] = xgboost_feature_importances_df[\"importance\"].rank(ascending=False)\n",
    "# lightgbm_feature_importances_df = pd.DataFrame(\n",
    "#     {\"var\": lightgbm_feature_importances.keys(), \"importance\": lightgbm_feature_importances.values()}\n",
    "# )\n",
    "# #lightgbm_feature_importances_df[\"importance\"] /= len(training_timeframe)\n",
    "# # lightgbm_feature_importances_df[\"rank_importance\"] = lightgbm_feature_importances_df[\"importance\"].rank(ascending=False)\n",
    "# # catboost_feature_importances_df = pd.DataFrame(\n",
    "# #     {\"var\": catboost_feature_importances.keys(), \"importance_catboost\": catboost_feature_importances.values()}\n",
    "# # )\n",
    "# # catboost_feature_importances_df[\"rank_importance\"] = catboost_feature_importances_df[\"importance_catboost\"].rank(ascending=False)\n",
    "# feature_importances_df = xgboost_feature_importances_df.merge(\n",
    "#     lightgbm_feature_importances_df,\n",
    "#     on=\"var\",\n",
    "#     how=\"inner\",\n",
    "#     suffixes=(\"_xgboost\", \"_lightgbm\")\n",
    "# )\n",
    "# # feature_importances_df = feature_importances_df.merge(\n",
    "# #     catboost_feature_importances_df,\n",
    "# #     on=\"var\",\n",
    "# #     how=\"inner\",\n",
    "# #     suffixes=(\"\", \"_catboost\")\n",
    "# # )\n",
    "# # feature_importances_df = feature_importances_df[[\"var\", \"rank_importance_xgboost\", \"rank_importance_lightgbm\", \"rank_importance_catboost\"]]\n",
    "# # feature_importances_df[\"rank\"] = 1/3 * (feature_importances_df[\"rank_importance_xgboost\"] + feature_importances_df[\"rank_importance_lightgbm\"] + feature_importances_df[\"rank_importance_catboost\"])\n",
    "# feature_importances_df[\"importance\"] = 1/2 * (feature_importances_df[\"importance_xgboost\"] + feature_importances_df[\"importance_lightgbm\"])\n",
    "# feature_importances_df = feature_importances_df.sort_values(by=\"importance\", ascending=False).reset_index().drop(\"index\", axis = 1)\n",
    "# feature_importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe565bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_xgboost_score = optuna.load_study(\n",
    "#     study_name = \"xgboost_2_4_101_1000_common_truncated_20_study\",\n",
    "#     storage = f\"sqlite:///xgboost_2_4_101_1000_common_truncated_20_study.db\"\n",
    "# ).best_value\n",
    "# best_lightgbm_score = optuna.load_study(\n",
    "#     study_name = \"lightgbm_2_4_101_1000_common_truncated_20_study\",\n",
    "#     storage = f\"sqlite:///lightgbm_2_4_101_1000_common_truncated_20_study.db\"\n",
    "# ).best_value\n",
    "# feature_importances_df[\"weighted_importance\"] = (best_xgboost_score * feature_importances_df[\"importance_xgboost\"] + best_lightgbm_score * feature_importances_df[\"importance_lightgbm\"]) / (best_xgboost_score + best_lightgbm_score)\n",
    "# feature_importances_df = feature_importances_df.sort_values(\"weighted_importance\", ascending=False, ignore_index=True)\n",
    "# feature_importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6e2f1",
   "metadata": {},
   "source": [
    "### Training model on test shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_popular_features_test = pd.read_parquet(\"data/cleaned/shuffled_popular_features_test.parquet\")\n",
    "\n",
    "# temporal_shuffled_popular_features_test = pd.read_parquet(\"data/cleaned/temporal_shuffled_popular_features_test.parquet\")\n",
    "\n",
    "# temporal_shuffled_X_test = pd.read_parquet(f\"data/cleaned/temporal_shuffled_cleaned_test_{feature_version}.parquet\")\n",
    "# temporal_shuffled_X_test = temporal_shuffled_X_test.drop(columns=[\"label\", \"submission_inx\"])\n",
    "\n",
    "# shuffled_X_test = pd.read_parquet(f\"data/cleaned/shuffled_cleaned_test_{feature_version}.parquet\")\n",
    "# shuffled_X_test = pd.concat([shuffled_X_test, shuffled_popular_features_test, temporal_shuffled_popular_features_test, temporal_shuffled_X_test], axis = 1)\n",
    "# shuffled_submission_inx = shuffled_X_test[\"submission_inx\"]\n",
    "# shuffled_X_test = shuffled_X_test.drop(columns=[\"label\", \"submission_inx\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
