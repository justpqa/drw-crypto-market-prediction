{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158f9a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justpqa/drw-crypto-market-prediction/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum n_jobs you can use: 12\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import multiprocessing\n",
    "max_n_jobs = multiprocessing.cpu_count()\n",
    "print(f\"Maximum n_jobs you can use: {max_n_jobs}\")\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nnmx\n",
    "import mlx.optimizers as optimmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779f7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_version = 2\n",
    "default_cv = 4\n",
    "# 1 for pc feature, \n",
    "# 2 for label correlation feature\n",
    "# 3 for best features based on combination rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45976d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_n_trees = 1000\n",
    "default_random_state = 101\n",
    "default_num_layers = 2\n",
    "random.seed(default_random_state)\n",
    "np.random.seed(default_random_state)\n",
    "mx.random.seed(default_random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7d631",
   "metadata": {},
   "source": [
    "### Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498f9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_features_train = pd.read_parquet(\"data/cleaned/popular_features_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f985312",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(f\"data/cleaned/cleaned_train_{feature_version}.parquet\")\n",
    "train_df = pd.concat([train_df, popular_features_train], axis = 1)\n",
    "train_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"])\n",
    "# train_df = train_df[train_df[\"timestamp\"].dt.month.isin([12, 1, 2])].reset_index().drop(\"index\", axis = 1)\n",
    "# X_train = train_df.drop(columns=[\"timestamp\", \"label\"])\n",
    "# Y_train = train_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c875ae5",
   "metadata": {},
   "source": [
    "### Train best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9b1a9",
   "metadata": {},
   "source": [
    "#### Get best features and training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2aebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['X862', 'X598', 'X863', 'X856', 'X612', 'X466', 'X533', 'X861', 'X445', 'X531', \n",
    "                 'X385', 'X23', 'X284', 'X465', 'X331', 'X95', 'X285', 'X31', 'X169', 'X137'] + \\\n",
    "                [\"volume\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"] + \\\n",
    "                [col for col in train_df.columns.tolist() if \"X\" not in col and col not in [\"timestamp\", \"label\"]]\n",
    "best_features = list(set(best_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b295efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2],\n",
       " [3, 4, 5, 6, 7, 8],\n",
       " [4, 5, 6, 7, 8, 9],\n",
       " [5, 6, 7, 8, 9, 10],\n",
       " [6, 7, 8, 9, 10, 11],\n",
       " [7, 8, 9, 10, 11, 12],\n",
       " [8, 9, 10, 11, 12, 1],\n",
       " [9, 10, 11, 12, 1, 2],\n",
       " [3, 4, 5],\n",
       " [4, 5, 6],\n",
       " [5, 6, 7],\n",
       " [6, 7, 8],\n",
       " [7, 8, 9],\n",
       " [8, 9, 10],\n",
       " [9, 10, 11],\n",
       " [10, 11, 12],\n",
       " [11, 12, 1],\n",
       " [12, 1, 2]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2]\n",
    "training_timeframe = [] \n",
    "for window_size in [12, 6, 3]:\n",
    "    for i in range(13 - window_size):\n",
    "        training_timeframe.append(months[i: i + window_size])\n",
    "training_timeframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82af05",
   "metadata": {},
   "source": [
    "#### Utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4885c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params_from_file(filename):\n",
    "    study = optuna.load_study(\n",
    "        study_name = filename,\n",
    "        storage = f\"sqlite:///{filename}.db\"\n",
    "    )\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64549233",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a05045",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Record does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: default_n_trees,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_categorical\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: default_random_state\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m best_params_xgboost \u001b[38;5;241m=\u001b[39m \u001b[43mget_best_params_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxgboost_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfeature_version\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdefault_cv\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdefault_random_state\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdefault_n_trees\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_common_truncated_20_study\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m best_params_xgboost:\n\u001b[1;32m      9\u001b[0m     params[p] \u001b[38;5;241m=\u001b[39m best_params_xgboost[p]\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mget_best_params_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_best_params_from_file\u001b[39m(filename):\n\u001b[0;32m----> 2\u001b[0m     study \u001b[38;5;241m=\u001b[39m \u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_study\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msqlite:///\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m~/drw-crypto-market-prediction/venv/lib/python3.9/site-packages/optuna/_convert_positional_args.py:134\u001b[0m, in \u001b[0;36mconvert_positional_args.<locals>.converter_decorator.<locals>.converter_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() got multiple values for arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduplicated_kwds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    132\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(inferred_kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/drw-crypto-market-prediction/venv/lib/python3.9/site-packages/optuna/study/study.py:1385\u001b[0m, in \u001b[0;36mload_study\u001b[0;34m(study_name, storage, sampler, pruner)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     study_name \u001b[38;5;241m=\u001b[39m study_names[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1380\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1381\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudy name was omitted but trying to load \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because that was the only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudy found in the storage.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m     )\n\u001b[0;32m-> 1385\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[43mStudy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudy_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpruner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpruner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mdirections) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     study\u001b[38;5;241m.\u001b[39msampler \u001b[38;5;241m=\u001b[39m samplers\u001b[38;5;241m.\u001b[39mNSGAIISampler()\n",
      "File \u001b[0;32m~/drw-crypto-market-prediction/venv/lib/python3.9/site-packages/optuna/study/study.py:87\u001b[0m, in \u001b[0;36mStudy.__init__\u001b[0;34m(self, study_name, storage, sampler, pruner)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy_name \u001b[38;5;241m=\u001b[39m study_name\n\u001b[1;32m     86\u001b[0m storage \u001b[38;5;241m=\u001b[39m storages\u001b[38;5;241m.\u001b[39mget_storage(storage)\n\u001b[0;32m---> 87\u001b[0m study_id \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_study_id_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_study_id \u001b[38;5;241m=\u001b[39m study_id\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage \u001b[38;5;241m=\u001b[39m storage\n",
      "File \u001b[0;32m~/drw-crypto-market-prediction/venv/lib/python3.9/site-packages/optuna/storages/_cached_storage.py:107\u001b[0m, in \u001b[0;36m_CachedStorage.get_study_id_from_name\u001b[0;34m(self, study_name)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_study_id_from_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, study_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_study_id_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/drw-crypto-market-prediction/venv/lib/python3.9/site-packages/optuna/storages/_rdb/storage.py:331\u001b[0m, in \u001b[0;36mRDBStorage.get_study_id_from_name\u001b[0;34m(self, study_name)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_study_id_from_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, study_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _create_scoped_session(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoped_session) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m--> 331\u001b[0m         study \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStudyModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_or_raise_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m         study_id \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mstudy_id\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m study_id\n",
      "File \u001b[0;32m~/drw-crypto-market-prediction/venv/lib/python3.9/site-packages/optuna/storages/_rdb/models.py:87\u001b[0m, in \u001b[0;36mStudyModel.find_or_raise_by_name\u001b[0;34m(cls, study_name, session)\u001b[0m\n\u001b[1;32m     85\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfind_by_name(study_name, session)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m study \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(NOT_FOUND_MSG)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Record does not exist.'"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": default_n_trees,\n",
    "    \"verbosity\": 0,\n",
    "    \"enable_categorical\": True,\n",
    "    \"random_state\": default_random_state\n",
    "}\n",
    "best_params_xgboost = get_best_params_from_file(f\"xgboost_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_20_study\")\n",
    "for p in best_params_xgboost:\n",
    "    params[p] = best_params_xgboost[p]\n",
    "\n",
    "xgbr_arr = []\n",
    "\n",
    "for i in tqdm(range(len(training_timeframe))):\n",
    "    temp = deepcopy(train_df)\n",
    "    temp = temp[temp[\"timestamp\"].dt.month.isin(training_timeframe[i])].reset_index().drop(\"index\", axis = 1)\n",
    "    X_train = temp.drop(columns=[\"timestamp\", \"label\"])\n",
    "    X_train = X_train[best_features]\n",
    "    Y_train = temp[\"label\"]\n",
    "    xgbr = XGBRegressor(**params)\n",
    "    xgbr.fit(X_train, Y_train)\n",
    "    xgbr_arr.append(xgbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915f46d",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b14c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": default_n_trees,\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": default_random_state,\n",
    "}\n",
    "best_params_lightgbm = get_best_params_from_file(f\"lightgbm_{feature_version}_{default_cv}_{default_random_state}_{default_n_trees}_common_truncated_20_study\")\n",
    "for p in best_params_lightgbm:\n",
    "    params[p] = best_params_lightgbm[p]\n",
    "\n",
    "lgbr_arr = []\n",
    "\n",
    "for i in tqdm(range(len(training_timeframe))):\n",
    "    temp = deepcopy(train_df)\n",
    "    temp = temp[temp[\"timestamp\"].dt.month.isin(training_timeframe[i])].reset_index().drop(\"index\", axis = 1)\n",
    "    X_train = temp.drop(columns=[\"timestamp\", \"label\"])\n",
    "    X_train = X_train[best_features]\n",
    "    Y_train = temp[\"label\"]\n",
    "    lgbr = LGBMRegressor(**params)\n",
    "    lgbr.fit(X_train, Y_train)\n",
    "    lgbr_arr.append(lgbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1482726",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f361db",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8056ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra code to \"reduce\" from float64 to float32\n",
    "def float64_to_float32(data):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        for col in data.columns:\n",
    "            data[col] = data[col].astype(\"float32\")\n",
    "    elif isinstance(data, pd.Series):\n",
    "        data = data.astype(\"float32\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dacf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_cv_to_mlx_cv(X_train, Y_train = None):\n",
    "    # Normalize forst\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.values)\n",
    "\n",
    "    # Convert to mlx format\n",
    "    X_train = mx.array(X_train)\n",
    "    if Y_train is not None:\n",
    "        Y_train = mx.array(Y_train.values)\n",
    "        return X_train, Y_train\n",
    "    else:\n",
    "        return X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40f197",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c395bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# We do not use the reset method this time so you have to create the model at each fold\n",
    "class MLPMLX(nnmx.Module):\n",
    "    def __init__(self, num_features, hidden_layers_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers & batchnorm\n",
    "        last_layer = num_features\n",
    "        self.layers = []\n",
    "        for current_layer in hidden_layers_size:\n",
    "            self.layers.append(nnmx.Linear(last_layer, current_layer))\n",
    "            last_layer = current_layer\n",
    "        self.layers.append(nnmx.Linear(last_layer, 1))\n",
    "\n",
    "        # Initialize activation\n",
    "        self.activation = nnmx.ReLU()\n",
    "\n",
    "        # Initialze dropout\n",
    "        self.dropout = nnmx.Dropout(p = dropout)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for inx, layer in enumerate(self.layers):\n",
    "            if inx == len(self.layers) - 1:\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                x = self.activation(x)\n",
    "                x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7856fc",
   "metadata": {},
   "source": [
    "Define batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70462fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for batch iteration\n",
    "def batch_iterate(batch_size, X, Y = None, shuffle = True):\n",
    "    if Y is not None:\n",
    "        for i in range(0, Y.size, batch_size):\n",
    "            X_curr = X[i: min(i + batch_size, Y.size), :]\n",
    "            Y_curr = Y[i: min(i + batch_size, Y.size)]\n",
    "            if shuffle:\n",
    "                inx_lst = mx.random.permutation(batch_size)\n",
    "                X_curr = X_curr[inx_lst, :]\n",
    "                Y_curr = Y_curr[inx_lst]\n",
    "            yield X_curr, Y_curr\n",
    "    else:\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_curr = X[i: min(i + batch_size, X.shape[0]), :]\n",
    "            yield X_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988601b",
   "metadata": {},
   "source": [
    "Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate function for train & eval step\n",
    "def train_mlp_mlx(model, loss_and_grad_fn, optimizer, X_train, Y_train, batch_size, num_epochs):\n",
    "    model.train()\n",
    "    for _ in tqdm(range(num_epochs)):\n",
    "        for (inputs, targets) in batch_iterate(batch_size, X_train, Y_train):\n",
    "            _, grads = loss_and_grad_fn(model, inputs, targets)\n",
    "            # Update the optimizer state and model parameters in a single call\n",
    "            optimizer.update(model, grads)\n",
    "            # Force a graph evaluation\n",
    "            mx.eval(model.parameters(), optimizer.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_mlx_lst(num_features, hidden_layers_size, dropout, lr, train_df, batch_size, num_epochs, best_features, training_timeframe):\n",
    "    mlp_mlx_arr = []\n",
    "    for _, timeframe in enumerate(training_timeframe):\n",
    "        # Get the data\n",
    "        temp = deepcopy(train_df)\n",
    "        temp = temp[temp[\"timestamp\"].dt.month.isin(timeframe)].reset_index().drop(\"index\", axis = 1)\n",
    "        X_train = temp.drop(columns=[\"timestamp\", \"label\"])\n",
    "        X_train = X_train[best_features]\n",
    "        Y_train = temp[\"label\"]\n",
    "\n",
    "        # Preprocess the data\n",
    "        # convert to float 32\n",
    "        X_train = float64_to_float32(X_train)\n",
    "        Y_train = float64_to_float32(Y_train)\n",
    "        # convert to mlx\n",
    "        X_train, Y_train = normal_cv_to_mlx_cv(X_train, Y_train)\n",
    "        \n",
    "        # initialize the model\n",
    "        mx.random.seed(default_random_state)\n",
    "        model = MLPMLX(num_features, hidden_layers_size, dropout)\n",
    "\n",
    "        # Initialize the loss function\n",
    "        def loss_fn(model, X, Y):\n",
    "            Y_pred = model(X).reshape(-1)\n",
    "            return mx.mean(nnmx.losses.mse_loss(Y_pred, Y))\n",
    "        loss_and_grad_fn = nnmx.value_and_grad(model, loss_fn)\n",
    "\n",
    "        # Reinitialize the optimizer\n",
    "        optimizer = optimmx.Adam(learning_rate = lr)\n",
    "\n",
    "        # Train the model\n",
    "        train_mlp_mlx(model, loss_and_grad_fn, optimizer, X_train, Y_train, batch_size, num_epochs)\n",
    "\n",
    "        # Add the model to model list\n",
    "        mlp_mlx_arr.append(model)\n",
    "    return mlp_mlx_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70040144",
   "metadata": {},
   "source": [
    "Define function for making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345699e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mlp_mlx(model, X_test, batch_size):\n",
    "    # Preprocess the data\n",
    "    # convert to float 32\n",
    "    X_test = float64_to_float32(X_test)\n",
    "    # convert to mlx\n",
    "    X_test = normal_cv_to_mlx_cv(X_test)\n",
    "    \n",
    "    outputs_all = np.zeros(0)\n",
    "    model.eval()\n",
    "    for inputs in batch_iterate(batch_size, X_test):\n",
    "        outputs = model(inputs).reshape(-1)\n",
    "        # convert back to numpy\n",
    "        outputs = np.array(outputs)\n",
    "        # Load to overall Y_test, Y_pred to calculate pearson score later\n",
    "        outputs_all = np.concatenate([outputs_all, outputs])\n",
    "    return outputs_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f73281",
   "metadata": {},
   "source": [
    "Train our model list based on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['X862', 'X598', 'X863', 'X856', 'X612', 'X466', 'X533', 'X861', 'X445', 'X531', \n",
    "                 'X385', 'X23', 'X284', 'X465', 'X331', 'X95', 'X285', 'X31', 'X169', 'X137'] \n",
    "best_features = list(set(best_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters for training\n",
    "best_params_mlp = get_best_params_from_file(f\"mlp_mlx_{feature_version}_{default_cv}_{default_random_state}_{default_num_layers}_common_truncated_{len(best_features)}_study\")\n",
    "num_features = len(best_features)\n",
    "num_layers = 3\n",
    "log_2_hidden_layers_size = []\n",
    "for i in range(num_layers):\n",
    "    log_2_hidden_layers_size.append(best_params_mlp[f\"log2_hidden_layer_{i}\"])\n",
    "hidden_layers_size = [2**l for l in log_2_hidden_layers_size]\n",
    "dropout = best_params_mlp[\"dropout\"]\n",
    "lr = best_params_mlp[\"lr\"]\n",
    "batch_size = best_params_mlp[\"batch_size\"]\n",
    "num_epochs = best_params_mlp[\"num_epochs\"]\n",
    "\n",
    "# Conduct training to get model list\n",
    "mlp_mlx_arr = train_mlp_mlx_lst(num_features, hidden_layers_size, dropout, lr, train_df, batch_size, num_epochs, best_features, training_timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since training of this model is expensive, try to save them somewhere\n",
    "with open(f\"mlp_mlx_{feature_version}_{default_cv}_{default_random_state}_{default_num_layers}_common_truncated_{len(best_features)}_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mlp_mlx_arr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9630cb",
   "metadata": {},
   "source": [
    "Making prediction & submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ee22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_features_test = pd.read_parquet(\"data/cleaned/popular_features_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlp_mlx model if needed\n",
    "with open(f\"mlp_mlx_{feature_version}_{default_cv}_{default_random_state}_{default_num_layers}_common_truncated_{len(best_features)}_model.pkl\", \"wb\") as f:\n",
    "    mlp_mlx_arr_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096457bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_parquet(f\"data/cleaned/cleaned_test_{feature_version}.parquet\")\n",
    "X_test = pd.concat([X_test, popular_features_test], axis = 1)\n",
    "X_test = X_test.drop(columns=[\"label\"])\n",
    "X_test = X_test[best_features]\n",
    "Y_pred = np.zeros(X_test.shape[0])\n",
    "for i in tqdm(range(len(training_timeframe))):\n",
    "    Y_pred += 1/3 * (xgbr_arr[i].predict(X_test) + lgbr_arr[i].predict(X_test) + predict_mlp_mlx(mlp_mlx_arr[i], X_test, batch_size))\n",
    "Y_pred /= len(training_timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a13fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": X_test.index + 1,\n",
    "    \"prediction\": Y_pred\n",
    "})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951e4a6",
   "metadata": {},
   "source": [
    "Analysis of contribution to submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_shap_values(model, X_test):\n",
    "#     explainer = shap.TreeExplainer(model)\n",
    "#     shap_values = explainer.shap_values(X_test)\n",
    "#     mean_abs_shap = np.mean(np.abs(shap_values), axis = 0)\n",
    "#     return mean_abs_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost_feature_importances = {}\n",
    "# lightgbm_feature_importances = {}\n",
    "\n",
    "# # only consider the fold with whole dataset to be comparable with cv scheme\n",
    "# features = xgbr_arr[0].feature_names_in_.tolist()\n",
    "# features_i = get_shap_values(xgbr_arr[i], X_test)\n",
    "# for inx, feat in enumerate(features):\n",
    "#     xgboost_feature_importances[feat] = xgboost_feature_importances.get(feat, 0) + features_i[inx]\n",
    "# features = lgbr_arr[0].feature_names_in_.tolist()\n",
    "# features_i = get_shap_values(lgbr_arr[i], X_test)\n",
    "# for inx, feat in enumerate(features):\n",
    "#     lightgbm_feature_importances[feat] = lightgbm_feature_importances.get(feat, 0) + features_i[inx]\n",
    "\n",
    "# xgboost_feature_importances_df = pd.DataFrame(\n",
    "#     {\"var\": xgboost_feature_importances.keys(), \"importance\": xgboost_feature_importances.values()}\n",
    "# )\n",
    "# #xgboost_feature_importances_df[\"importance\"] /= len(training_timeframe)\n",
    "# # xgboost_feature_importances_df[\"rank_importance\"] = xgboost_feature_importances_df[\"importance\"].rank(ascending=False)\n",
    "# lightgbm_feature_importances_df = pd.DataFrame(\n",
    "#     {\"var\": lightgbm_feature_importances.keys(), \"importance\": lightgbm_feature_importances.values()}\n",
    "# )\n",
    "# #lightgbm_feature_importances_df[\"importance\"] /= len(training_timeframe)\n",
    "# # lightgbm_feature_importances_df[\"rank_importance\"] = lightgbm_feature_importances_df[\"importance\"].rank(ascending=False)\n",
    "# # catboost_feature_importances_df = pd.DataFrame(\n",
    "# #     {\"var\": catboost_feature_importances.keys(), \"importance_catboost\": catboost_feature_importances.values()}\n",
    "# # )\n",
    "# # catboost_feature_importances_df[\"rank_importance\"] = catboost_feature_importances_df[\"importance_catboost\"].rank(ascending=False)\n",
    "# feature_importances_df = xgboost_feature_importances_df.merge(\n",
    "#     lightgbm_feature_importances_df,\n",
    "#     on=\"var\",\n",
    "#     how=\"inner\",\n",
    "#     suffixes=(\"_xgboost\", \"_lightgbm\")\n",
    "# )\n",
    "# # feature_importances_df = feature_importances_df.merge(\n",
    "# #     catboost_feature_importances_df,\n",
    "# #     on=\"var\",\n",
    "# #     how=\"inner\",\n",
    "# #     suffixes=(\"\", \"_catboost\")\n",
    "# # )\n",
    "# # feature_importances_df = feature_importances_df[[\"var\", \"rank_importance_xgboost\", \"rank_importance_lightgbm\", \"rank_importance_catboost\"]]\n",
    "# # feature_importances_df[\"rank\"] = 1/3 * (feature_importances_df[\"rank_importance_xgboost\"] + feature_importances_df[\"rank_importance_lightgbm\"] + feature_importances_df[\"rank_importance_catboost\"])\n",
    "# feature_importances_df[\"importance\"] = 1/2 * (feature_importances_df[\"importance_xgboost\"] + feature_importances_df[\"importance_lightgbm\"])\n",
    "# feature_importances_df = feature_importances_df.sort_values(by=\"importance\", ascending=False).reset_index().drop(\"index\", axis = 1)\n",
    "# feature_importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe565bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_xgboost_score = optuna.load_study(\n",
    "#     study_name = \"xgboost_2_4_101_1000_common_truncated_20_study\",\n",
    "#     storage = f\"sqlite:///xgboost_2_4_101_1000_common_truncated_20_study.db\"\n",
    "# ).best_value\n",
    "# best_lightgbm_score = optuna.load_study(\n",
    "#     study_name = \"lightgbm_2_4_101_1000_common_truncated_20_study\",\n",
    "#     storage = f\"sqlite:///lightgbm_2_4_101_1000_common_truncated_20_study.db\"\n",
    "# ).best_value\n",
    "# feature_importances_df[\"weighted_importance\"] = (best_xgboost_score * feature_importances_df[\"importance_xgboost\"] + best_lightgbm_score * feature_importances_df[\"importance_lightgbm\"]) / (best_xgboost_score + best_lightgbm_score)\n",
    "# feature_importances_df = feature_importances_df.sort_values(\"weighted_importance\", ascending=False, ignore_index=True)\n",
    "# feature_importances_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
